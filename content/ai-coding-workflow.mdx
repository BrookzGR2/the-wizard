---
title: "AI Coding Workflow"
date: "2026-02-05"
description: "A universal, state-of-the-art workflow for AI-assisted software development. The brain + muscle architecture for building software with AI agents."
tags: ["ai", "coding", "workflow", "automation", "guide"]
coverImage: "/images/articles/ai-coding-workflow.png"
readingTime: "15 min read"
---

Modern AI development uses a **brain + muscle** architecture. The brain plans and orchestrates. The muscles execute. This guide covers everything you need to build software with AI agents effectively.

## The Big Picture

```
┌─────────────────────────────────────────────────────────────────┐
│                         YOU (Human)                             │
│              Decisions • Reviews • Approvals                    │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                   ORCHESTRATOR (Brain)                          │
│   Claude Opus 4.5 or similar frontier model                     │
│   Plans • Specs • Dispatches • Reviews • Coordinates            │
└─────────────────────────────────────────────────────────────────┘
                              │
              ┌───────────────┼───────────────┐
              ▼               ▼               ▼
┌───────────────────┐ ┌───────────────┐ ┌───────────────────┐
│   QUICK CODING    │ │  LONG-HORIZON │ │   SPECIALIZED     │
│   GPT-5-Codex     │ │ GPT-5.3-Codex │ │   Gemini 3 Pro    │
│   Interactive     │ │  Walk-away    │ │   UI/Styling      │
│   < 30 min        │ │  2-8hr runs   │ │   Research tools  │
└───────────────────┘ └───────────────┘ └───────────────────┘
```

### Model Selection Guide

| Task Type | Best Model | Why |
|-----------|------------|-----|
| **Planning & orchestration** | Claude Opus 4.5 | Best reasoning, prompt engineering, judgment |
| **Quick coding (< 30 min)** | GPT-5-Codex | Fast, interactive, good for iteration |
| **Complex features (hours)** | GPT-5.3-Codex | True autonomy, better judgment under ambiguity |
| **UI/Styling** | Gemini 3 Pro | Best visual design and CSS capabilities |
| **Web research** | Perplexity Sonar | Real-time search with citations |
| **Image analysis** | Gemini 2.0 Flash | Best vision model for the cost |

### When to Use Which Agent

**Use interactive (Codex CLI) when:**
- Quick fixes, small tasks
- You want to watch and guide
- Exploring or prototyping
- Learning how something works

**Use long-horizon (GPT-5.3-Codex) when:**
- Complex, multi-file features
- Well-defined tasks with clear validation
- Overnight / walk-away work
- You want it done right, not fast

> **Key insight:** GPT-5.3-Codex with validation tests can run for 8+ hours without losing the thread. The unlock is giving it clear pass/fail criteria upfront.

---

## The Universal Workflow

Every task follows this pattern:

**DEFINE → VALIDATE → EXECUTE → VERIFY → SHIP**

### Step 1: Define

Write a clear spec before any code. Include:

- **Outcome**: What does success look like?
- **Acceptance criteria**: Bullet list of requirements
- **Non-goals**: What are we explicitly NOT doing?
- **Constraints**: Tech stack, performance, security requirements

**Example spec:**

```markdown
## Feature: User Authentication

### Outcome
Users can sign up, log in, and reset passwords.

### Acceptance Criteria
- [ ] Email/password signup with validation
- [ ] Secure login with session management
- [ ] Password reset via email link
- [ ] Rate limiting on auth endpoints

### Non-Goals
- No OAuth/social login (v2)
- No 2FA (v2)

### Constraints
- Use existing Supabase auth
- Follow existing UI patterns
```

### Step 2: Validate (The Unlock)

**Write tests BEFORE implementation.** This is the single biggest unlock for autonomous coding.

> "If you want full autonomy, there is one approach that dominates everything else: give the model strong validation and tests up front." — Matt Shumer

The agent iterates until tests pass. No tests = no guardrails = drift.

**Test types to include:**
- Unit tests for business logic
- Integration tests for data flows
- E2E tests for critical user paths

### Step 3: Execute

Choose execution mode based on task complexity:

| Mode | When | How |
|------|------|-----|
| **Interactive** | Quick tasks, learning | Codex CLI with real-time feedback |
| **Delegated** | Clear tasks, busy | Kick off agent, check back later |
| **Overnight** | Complex features | Spec + tests → long-horizon agent → morning PR |

**For long-horizon runs:**
- Use Extra High reasoning mode ("do it right, take your time")
- Set clear timeout expectations
- Ensure tests are comprehensive

### Step 4: Verify

Never ship AI-generated code without verification:

1. **Run the tests** — They should all pass
2. **Read the diff** — Understand what changed
3. **Check for the "almost right" trap** — 95% correct with subtle bugs
4. **Cross-validate if unsure** — Run code past a second model

### Step 5: Ship

- Create PR on feature branch (never push directly to main)
- Human reviews and approves
- Merge and deploy

---

## Git Worktrees

Worktrees let you run multiple branches simultaneously in separate directories. Essential for parallel AI work.

### Why Worktrees Matter for AI

- **Isolation**: AI works in separate directory, your main work stays untouched
- **Parallelism**: Run multiple AI tasks simultaneously
- **Safety**: Easy to discard entire worktree if approach fails
- **Comparison**: Compare AI implementations side-by-side

### Recommended Structure

```
~/projects/
├── my-app/                    # Main worktree (your work)
├── my-app-ai-feature/         # AI implementing feature
├── my-app-ai-refactor/        # AI refactoring task
└── my-app-ai-experiment/      # AI exploring approach
```

### Best Practices

| Practice | Why |
|----------|-----|
| One worktree per task | Clean isolation, easy cleanup |
| Clear naming: `{project}-ai-{task}` | Easy to identify purpose |
| Commit frequently | Granular commits enable cherry-picking |
| No real credentials | Use `.env.example` templates |
| Clean up when done | Don't accumulate stale worktrees |

---

## Automation

### What Should Be Automated

| Task | Automate? | How |
|------|-----------|-----|
| Running tests | ✅ Yes | CI/CD on every push |
| Linting/formatting | ✅ Yes | Pre-commit hooks |
| Type checking | ✅ Yes | CI pipeline |
| Deploy to staging | ✅ Yes | Auto-deploy on PR merge |
| Deploy to prod | ⚠️ Semi | Human approval gate |
| Code review | ⚠️ Semi | AI reviews, human approves |
| Security scanning | ✅ Yes | Automated in CI |

### Closed-Loop Development

The goal: Agent can **deploy → check production → tail logs → iterate**.

This enables agents to verify their own work in production, catching issues that only appear in real environments.

### Nightly Development Sessions

For serious compound progress, schedule autonomous dev time:

1. Agent checks project backlog
2. Picks highest-priority, well-defined tasks
3. Writes specs and tests for each
4. Runs long-horizon coding agent
5. Creates PRs for morning review

---

## Tools & Extensions

### Sub-Agents

Sub-agents are spawned instances for parallel work:

- **Parallel research**: Multiple agents analyze different parts simultaneously
- **Specialized tasks**: One for logs, one for database, one for code review
- **Token efficiency**: Each sub-agent has its own context budget

**Key rule:** Read operations parallelize well. Write operations stay sequential to avoid conflicts.

### MCP (Model Context Protocol)

MCP is the emerging standard for connecting AI agents to external tools and data.

**What it enables:**
- Dynamic tool discovery (agent finds available tools at runtime)
- Standardized tool invocation
- Stateful sessions across tools
- Security and consent controls

**Resources:** [modelcontextprotocol.io](https://modelcontextprotocol.io)

### Skills

Skills are packaged capabilities for specific tasks — plugins for your agent.

**Finding skills:**
- [skills.sh](https://skills.sh) — Skill directory
- [clawdhub.com](https://clawdhub.com) — OpenClaw skill hub

---

## Debugging

Debugging AI-generated code requires a structured approach:

**REPRODUCE → ISOLATE → EXPLAIN → FIX → VERIFY**

### The Steps

1. **Reproduce**: Get minimal reproduction — exact steps, expected vs actual, error messages
2. **Isolate**: Narrow down — which file/function? Which input triggers it?
3. **Explain**: Have the agent walk through the code line by line
4. **Fix**: Apply fix with context — bug, root cause, change, verification
5. **Verify**: Run reproduction (should pass), run full test suite (no regressions)

### Cross-Model Validation

When stuck, get a second opinion. Share code + error with different model, compare diagnoses, test fixes.

**No single model is fully reliable.** Cross-validation catches blind spots.

### Common AI Code Issues

| Issue | Signs | Prevention |
|-------|-------|------------|
| Off-by-one errors | Loops, array access | Edge case tests |
| Missing null checks | Runtime crashes | Type checking, tests |
| Security flaws | SQL injection, XSS | Security scanning |
| Performance issues | Slow operations | Load testing |
| Incomplete error handling | Silent failures | Error path tests |

---

## Context Management

With 200K token windows (1M in beta), context management is less critical but still matters for efficiency and cost.

### Practical Guidelines

| Context Level | Action |
|---------------|--------|
| < 30% used | Work freely |
| 30-60% used | Monitor, consider compaction |
| > 60% used | Compact or start fresh session |

### What to Keep vs Offload

**Keep:**
- Current task spec and acceptance criteria
- Relevant code files being modified
- Recent error messages and fixes
- Key architectural decisions

**Offload:**
- Completed subtasks (summarize)
- Exploration that didn't pan out
- Verbose logs (extract key info)

---

## Cost Management

### How to Think About Cost

**The principle:** Use the best model that's cost-effective for the task. Don't cheap out on important work, don't overspend on simple tasks.

| Task Complexity | Model Choice | Why |
|-----------------|--------------|-----|
| Simple/routine | Cheaper, faster models | Speed matters, quality sufficient |
| Important features | Best available | Quality matters, cost is investment |
| Long-horizon overnight | Best available | Paying for reliability, not speed |
| Exploration/prototyping | Cheaper models | You'll likely redo it anyway |

### Current Pricing (Feb 2026)

| Model | Input (per 1M tokens) | Output (per 1M tokens) |
|-------|----------------------|------------------------|
| Claude Opus 4.5 | $5.00 | $25.00 |
| GPT-5-Codex | $1.25 | $10.00 |
| GPT-5.3-Codex | ~$1.25 | ~$10.00 (est.) |
| Gemini 2.0 Flash | $0.10 | $0.40 |
| Perplexity Sonar | ~$1.00/M | — |

### Example Task Costs

| Task | Est. Tokens | Est. Cost |
|------|-------------|-----------|
| Quick bug fix | 50K | $0.50-1.00 |
| Small feature | 200K | $2-4 |
| Complex feature (overnight) | 2M | $15-25 |
| Full day interactive | 500K | $5-10 |

### How to Save Money

1. **Write good specs** — Clear requirements = fewer iterations
2. **Use the right model** — Don't use Opus for simple tasks
3. **Batch small tasks** — Combine fixes into one session
4. **Write tests first** — Agent converges faster
5. **Use caching** — Prompt caching cuts repeated costs
6. **Monitor usage** — Identify wasteful patterns
7. **Start smaller, escalate** — Try cheaper model first

### When NOT to Optimize

- **Production bugs** — Fix it right, fix it once
- **Security code** — Don't cut corners
- **Architecture decisions** — Worth the extra reasoning
- **Overnight runs** — Reliability > saving $5

**Bottom line:** Token costs are cheap compared to your time. A $20 overnight run that saves you 4 hours is a great deal.

---

## Configuration

### Codex CLI Setup

```bash
# Install
npm install -g @openai/codex

# Authenticate
codex auth  # Browser-based login
# OR
export OPENAI_API_KEY="sk-..."
```

**Config file (~/.codex/config.toml):**

```toml
# Model selection
model = "gpt-5-codex"  # or "gpt-5.3-codex"

# Safety settings
sandbox = "workspace-write"
ask_for_approval = "on-request"

# For full autonomy:
# ask_for_approval = "never"
```

### Reasoning Modes (GPT-5.3-Codex)

| Mode | Use Case | Speed |
|------|----------|-------|
| Low | Simple, clear tasks | Fastest |
| Medium | Default, balanced | Balanced |
| High | Complex, important | Slower |
| Extra High | Walk-away, "do it right" | Slowest, most reliable |

### Deployment Platforms

| Platform | Best For | Key Feature |
|----------|----------|-------------|
| Vercel | Frontend, Next.js | Instant deploys, edge functions |
| Railway | Backend, Docker | Containers, closed-loop deploys |

**Hybrid approach:** Frontend → Vercel, Backend/APIs → Railway

---

## Resources

### Documentation

| Tool | Docs |
|------|------|
| Codex CLI | [developers.openai.com/codex/cli](https://developers.openai.com/codex/cli/) |
| MCP Spec | [modelcontextprotocol.io](https://modelcontextprotocol.io) |
| Git Worktrees | [git-scm.com/docs/git-worktree](https://git-scm.com/docs/git-worktree) |

### Skill Directories

| Directory | URL |
|-----------|-----|
| Skills.sh | [skills.sh](https://skills.sh) |
| ClawdHub | [clawdhub.com](https://clawdhub.com) |

### Learning Resources

| Topic | Resource |
|-------|----------|
| Worktrees + AI | [dev.to guide](https://dev.to/bhaidar/supercharge-your-ai-coding-workflow-a-complete-guide-to-git-worktrees-with-claude-code-60m) |
| Context engineering | [jxnl.co/writing](https://jxnl.co/writing/2025/08/29/context-engineering-slash-commands-subagents/) |
| GPT-5.3 review | [shumer.dev](https://shumer.dev/gpt53-codex-review) |

---

## Core Principles

1. **Validation first** — Tests are the unlock for autonomous coding
2. **Right tool for the job** — Quick work vs long-horizon vs specialized
3. **Human in the loop** — AI proposes, human approves (especially prod)
4. **Compound progress** — Small consistent gains beat sporadic heroics
5. **Trust but verify** — AI is good, not infallible

---

*This workflow evolves. Update it as you learn what works.*
